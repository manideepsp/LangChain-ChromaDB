{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n"
     ]
    }
   ],
   "source": [
    "%pip install --q unstructured langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured[all-docs] in d:\\anaconda\\envs\\myllm\\lib\\site-packages (0.16.11)\n",
      "Requirement already satisfied: chardet in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (3.0.4)\n",
      "Requirement already satisfied: filetype in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (0.4.27)\n",
      "Requirement already satisfied: lxml in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (5.2.2)\n",
      "Requirement already satisfied: nltk in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (3.9.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (4.8.2)\n",
      "Requirement already satisfied: emoji in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (0.5.14)\n",
      "Requirement already satisfied: python-iso639 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (2024.10.22)\n",
      "Requirement already satisfied: langdetect in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (3.11.0)\n",
      "Requirement already satisfied: backoff in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (4.11.0)\n",
      "Requirement already satisfied: unstructured-client in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (0.28.1)\n",
      "Requirement already satisfied: wrapt in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (1.16.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (4.66.4)\n",
      "Requirement already satisfied: psutil in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (0.0.1)\n",
      "Requirement already satisfied: html5lib in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (1.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (3.3)\n",
      "Collecting unstructured-inference==0.8.1 (from unstructured[all-docs])\n",
      "  Downloading unstructured_inference-0.8.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting google-cloud-vision (from unstructured[all-docs])\n",
      "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pi-heif (from unstructured[all-docs])\n",
      "  Downloading pi_heif-0.21.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pikepdf (from unstructured[all-docs])\n",
      "  Downloading pikepdf-9.4.2-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: markdown in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (3.6)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (2.2.2)\n",
      "Requirement already satisfied: pypdf in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (5.1.0)\n",
      "Collecting openpyxl (from unstructured[all-docs])\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pdfminer.six in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (20191110)\n",
      "Collecting pdf2image (from unstructured[all-docs])\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting effdet (from unstructured[all-docs])\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: xlrd in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (1.2.0)\n",
      "Collecting onnx (from unstructured[all-docs])\n",
      "  Downloading onnx-1.17.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pypandoc (from unstructured[all-docs])\n",
      "  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: python-docx>=1.1.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured[all-docs]) (1.1.2)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
      "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting layoutparser (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: python-multipart in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (0.0.9)\n",
      "Requirement already satisfied: huggingface-hub in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (0.26.3)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.10.0.84)\n",
      "Requirement already satisfied: onnxruntime>=1.17.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (1.20.1)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (3.9.0)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (2.5.1)\n",
      "Collecting timm (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading timm-1.0.12-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: transformers>=4.25.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.47.1)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (10.3.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (3.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]) (23.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
      "Collecting torchvision (from effdet->unstructured[all-docs])\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[all-docs])\n",
      "  Downloading pycocotools-2.0.8-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (2.36.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-vision->unstructured[all-docs])\n",
      "  Using cached proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (4.25.3)\n",
      "Requirement already satisfied: six>=1.9 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from html5lib->unstructured[all-docs]) (1.12.0)\n",
      "Requirement already satisfied: webencodings in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n",
      "Requirement already satisfied: click in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from nltk->unstructured[all-docs]) (2024.5.15)\n",
      "Collecting et-xmlfile (from openpyxl->unstructured[all-docs])\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pandas->unstructured[all-docs]) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
      "Requirement already satisfied: pycryptodome in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pdfminer.six->unstructured[all-docs]) (3.20.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pdfminer.six->unstructured[all-docs]) (2.4.0)\n",
      "Requirement already satisfied: Deprecated in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pikepdf->unstructured[all-docs]) (1.2.14)\n",
      "Requirement already satisfied: olefile in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests->unstructured[all-docs]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests->unstructured[all-docs]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests->unstructured[all-docs]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests->unstructured[all-docs]) (2024.2.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from tqdm->unstructured[all-docs]) (0.4.6)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (42.0.7)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.9.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (2.9.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured[all-docs]) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.64.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (0.14.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.1)\n",
      "Requirement already satisfied: coloredlogs in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (24.3.25)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (3.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[all-docs]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[all-docs]) (2.23.4)\n",
      "Requirement already satisfied: safetensors in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from timm->unstructured-inference==0.8.1->unstructured[all-docs]) (0.4.5)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (2024.6.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (75.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from transformers>=4.25.1->unstructured-inference==0.8.1->unstructured[all-docs]) (0.21.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.1)\n",
      "Collecting iopath (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pdfplumber (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured[all-docs]) (2.22)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (10.0)\n",
      "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from jinja2->torch->unstructured-inference==0.8.1->unstructured[all-docs]) (2.1.5)\n",
      "Collecting pdfminer.six (from unstructured[all-docs])\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pyreadline3 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (3.5.4)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from portalocker->iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (305.1)\n",
      "Downloading unstructured_inference-0.8.1-py3-none-any.whl (48 kB)\n",
      "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
      "Downloading onnx-1.17.0-cp312-cp312-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 7.3/14.5 MB 37.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.4/14.5 MB 37.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 31.5 MB/s eta 0:00:00\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pi_heif-0.21.0-cp312-cp312-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading pikepdf-9.4.2-cp312-cp312-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.5/3.5 MB 29.2 MB/s eta 0:00:00\n",
      "Downloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading pycocotools-2.0.8-cp312-cp312-win_amd64.whl (83 kB)\n",
      "Downloading timm-1.0.12-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "   ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 13.1/19.2 MB 68.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.2/19.2 MB 50.4 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 86.7 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 38.2 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 57.4 MB/s eta 0:00:00\n",
      "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime, iopath\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144578 sha256=e04b15a628996d10e7a446e20b8729bf69978a471e84703815f6ecc339f9007e\n",
      "  Stored in directory: c:\\users\\manideep s\\appdata\\local\\pip\\cache\\wheels\\1f\\be\\48\\13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "  Building wheel for iopath (setup.py): started\n",
      "  Building wheel for iopath (setup.py): finished with status 'done'\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31537 sha256=14dbf5da43ac4242d959c5d2cafe52a2b85bf5e7be258b415ce53c694c83f28c\n",
      "  Stored in directory: c:\\users\\manideep s\\appdata\\local\\pip\\cache\\wheels\\7c\\96\\04\\4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
      "Successfully built antlr4-python3-runtime iopath\n",
      "Installing collected packages: antlr4-python3-runtime, unstructured.pytesseract, python-pptx, pypdfium2, pypandoc, proto-plus, portalocker, pi-heif, pdf2image, onnx, omegaconf, et-xmlfile, pikepdf, openpyxl, iopath, grpcio-status, torchvision, pycocotools, pdfminer.six, google-api-core, timm, pdfplumber, layoutparser, google-cloud-vision, effdet, unstructured-inference\n",
      "  Attempting uninstall: python-pptx\n",
      "    Found existing installation: python-pptx 0.6.23\n",
      "    Uninstalling python-pptx-0.6.23:\n",
      "      Successfully uninstalled python-pptx-0.6.23\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20191110\n",
      "    Uninstalling pdfminer.six-20191110:\n",
      "      Successfully uninstalled pdfminer.six-20191110\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 effdet-0.4.1 et-xmlfile-2.0.0 google-api-core-2.24.0 google-cloud-vision-3.9.0 grpcio-status-1.62.3 iopath-0.1.10 layoutparser-0.3.4 omegaconf-2.3.0 onnx-1.17.0 openpyxl-3.1.5 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pi-heif-0.21.0 pikepdf-9.4.2 portalocker-3.0.0 proto-plus-1.25.0 pycocotools-2.0.8 pypandoc-1.14 pypdfium2-4.30.1 python-pptx-1.0.2 timm-1.0.12 torchvision-0.20.1 unstructured-inference-0.8.1 unstructured.pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n"
     ]
    }
   ],
   "source": [
    "%pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-community) (1.4.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-community) (0.5.14)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.13 (from langchain-community)\n",
      "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langsmith<0.3,>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.2.6-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-community) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain<0.4.0,>=0.3.13->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\anaconda\\envs\\myllm\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 2.4/2.5 MB 26.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
      "Downloading langsmith-0.2.6-py3-none-any.whl (325 kB)\n",
      "Downloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
      "Downloading langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: httpx-sse, pydantic-settings, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.62\n",
      "    Uninstalling langsmith-0.1.62:\n",
      "      Successfully uninstalled langsmith-0.1.62\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.1\n",
      "    Uninstalling langchain-core-0.2.1:\n",
      "      Successfully uninstalled langchain-core-0.2.1\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.0\n",
      "    Uninstalling langchain-text-splitters-0.2.0:\n",
      "      Successfully uninstalled langchain-text-splitters-0.2.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.0\n",
      "    Uninstalling langchain-0.2.0:\n",
      "      Successfully uninstalled langchain-0.2.0\n",
      "Successfully installed httpx-sse-0.4.0 langchain-0.3.13 langchain-community-0.3.13 langchain-core-0.3.28 langchain-text-splitters-0.3.4 langsmith-0.2.6 pydantic-settings-2.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tesseract\n",
      "  Downloading tesseract-0.1.3.tar.gz (45.6 MB)\n",
      "     ---------------------------------------- 0.0/45.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 4.5/45.6 MB 30.0 MB/s eta 0:00:02\n",
      "     -------- ------------------------------ 10.2/45.6 MB 27.7 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 14.7/45.6 MB 26.4 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 20.4/45.6 MB 25.4 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 25.4/45.6 MB 25.2 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 30.1/45.6 MB 25.5 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 35.7/45.6 MB 25.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 39.8/45.6 MB 24.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 43.3/45.6 MB 23.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  45.4/45.6 MB 22.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 45.6/45.6 MB 20.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: tesseract\n",
      "  Building wheel for tesseract (setup.py): started\n",
      "  Building wheel for tesseract (setup.py): finished with status 'done'\n",
      "  Created wheel for tesseract: filename=tesseract-0.1.3-py3-none-any.whl size=45562569 sha256=4f8e8db02211db5aac66ba2a0dd0be9e6dce75f9f62b6b411b7c261a60f98be8\n",
      "  Stored in directory: c:\\users\\manideep s\\appdata\\local\\pip\\cache\\wheels\\13\\1f\\8e\\2d6c0e358fd6d01ca80ecd9185a374bcda35879f4fec727242\n",
      "Successfully built tesseract\n",
      "Installing collected packages: tesseract\n",
      "Successfully installed tesseract-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n"
     ]
    }
   ],
   "source": [
    "%pip install tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of Required Packages\n",
    "\n",
    "We start by installing the necessary packages for our project.\n",
    "These include `unstructured`, `langchain`, and `langchain-community`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from multiprocessing import Pool\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"SubjectBooks_PDF/Manideep_Sriperambudhuru_Resume 2.pdf\"\n",
    "\n",
    "# loader = PyMuPDFLoader(local_path)\n",
    "# data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader(local_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manideep Sriperambudhuru LinkedIn : manideepsp\\n\\nEmail : manideepsp.16@gmail.com Mobile : +91 6300-519-475\\n\\nEducation\\n\\nCMR Institute of Technology\\n\\nHyderabad, India Aug. 2019 – July. 2023\\n\\nBachelor of Technology in Mechanical Engineering; GPA: 8.46/10.0 Completed a major project on the Evaluation of Mechanical Properties of Aluminum-Epoxy Composite, involving the selection, preparation, and evaluation of the composite based on weight ratio, focusing on properties like tensile strength, hardness, and impact resistance.\\n\\nExperience\\n\\nCognine\\n\\nHyderabad, India Apr 2023 - Present ◦ Data Integration and ETL Development: Designed, developed, and maintained robust ETL workflows using Azure Data Factory (ADF) and Databricks. Automated the extraction, transformation, and loading of large volumes of data from various sources (SQL Server, cloud-based systems) into centralized data platforms. Ensured the accuracy, integrity, and performance of data pipelines by implementing best practices for error handling, data cleansing, and transformation.\\n\\nAssociate Software Engineer (Data Engineer)\\n\\nData Warehousing and Business Intelligence: Built and maintained scalable data warehouses by optimizing data models and schema designs to support reporting and analytics. Created interactive and insightful dashboards using Power BI and Tableau to provide key business insights, enhancing data-driven decision-making. Collaborated with stakeholders to gather requirements and translate them into actionable reports.\\n\\nData Transformation and Processing: Performed advanced data transformations using PySpark and SparkSQL within Databricks. Developed and optimized complex SQL queries and stored procedures to improve performance, ensuring efficient data retrieval, aggregation, and reporting.\\n\\nAutomation and Workflow Optimization: Automated repetitive tasks and processes using Power Platform tools like Power Automate and PowerApps to streamline workflows, reduce manual intervention, and enhance operational efficiency. Integrated data solutions using Dataverse API for secure and seamless interaction with backend systems.\\n\\nAgile Development and Collaboration: Followed Agile methodologies, managing sprint planning, backlog grooming, and daily standups using Azure DevOps for version control and collaboration. Worked closely with cross-functional teams, including business analysts, data scientists, and developers, to ensure alignment with project goals and timely delivery.\\n\\nProblem Solving and Optimization: Identified bottlenecks and implemented performance optimization strategies to ensure scalable and maintainable data solutions. Collaborated with teams to troubleshoot and resolve complex data issues, improving system reliability and reducing downtime.\\n\\nDocumentation and Knowledge Sharing: Created and maintained technical documentation to ensure clear understanding of data workflows, ETL processes, and reporting solutions. Provided knowledge sharing sessions and training to team members to enhance data literacy and ensure best practices are followed.\\n\\nProjects\\n\\nData Warehousing\\n\\nEmployee Database for Data Warehousing: Developed a centralized employee database application using ASP.NET Core MVC and Entity Framework Core to support data warehousing integration. Designed an optimized database schema in SQL Server to enable efficient querying and analytical workloads. Implemented CRUD operations with LINQ queries, leveraging advanced features like filtering, grouping, and aggregations. Ensured modularity and testability using Dependency Injection for service integration. Added robust role-based access control with ASP.NET Core Identity, enhancing security for sensitive operations. Created responsive and dynamic web pages using Razor Views, Bootstrap, and JavaScript. Prepared the backend for seamless integration with business intelligence tools, improving scalability and performance.\\n\\nETL & BI Pipelines\\n\\nAzure Data Solutions and Business Intelligence: Designed and implemented scalable ETL pipelines in Azure Data Factory (ADF) to automate data ingestion, transformation, and movement across systems. Optimized pipeline performance with robust error handling and logging. Developed interactive dashboards using Power BI for real-time business insights, leveraging efficient data modeling to reduce report load times. Integrated Power Platform tools such as PowerApps and Power Automate for ad hoc data manipulation and process automation. Enhanced system performance by writing advanced SQL queries and stored procedures to handle complex data transformations.\\n\\nInovalon Project: Data Integration and Transformation: Architected a robust data integration solution using Azure Data Factory (ADF) for orchestrating ETL workflows. Performed advanced transformations in Databricks using PySpark and SparkSQL to process large datasets efficiently. Designed optimized SQL queries for extracting, transforming, and loading data from SQL Server to downstream systems. Automated end-to-end pipelines for continuous data flow and reporting. Developed customized dashboards and reports in Tableau and Power BI to visualize critical business metrics, improving decision-making for stakeholders.\\n\\nEnd-to-End ETL BI Pipeline with GCP: Designed and implemented an end-to-end ETL pipeline using Google Cloud Platform (GCP) services. Orchestrated workflows with Apache Airflow, enabling scheduled data extraction and transformation. Utilized GCP Dataproc for large-scale data processing with Spark. Implemented a multi-layer architecture (Bronze, Silver, Gold) for optimized transformations and analytics. Developed dynamic dashboards using GCP Looker, offering actionable insights to stakeholders. Automated pipeline monitoring and logging for reliability and operational efficiency.\\n\\nMachine Learning & AI\\n\\nSentiment Analysis with ANN (IMDB Dataset): Developed an Artificial Neural Network (ANN) using TensorFlow and Keras to classify IMDB movie reviews as positive or negative. Preprocessed text data using tokenization, padding, and embeddings to convert reviews into numerical format. Designed the ANN with an embedding layer for feature extraction, dense layers for learning, and output layers for binary classification. Achieved high accuracy (89%) through iterative model optimization, hyperparameter tuning, and regularization. Implemented evaluation strategies like accuracy, loss monitoring, and confusion matrix analysis.\\n\\nImage Classification with CNN (Custom Dataset): Built an Image Classification model using a Convolutional Neural Network (CNN) with TensorFlow and Keras. Designed a multi-layer architecture with convolutional, pooling, and dense layers to extract and classify features from images. Trained the model on a custom dataset, achieving high classification accuracy. Applied data augmentation techniques (rotation, flipping, scaling) to enhance model generalization and prevent overfitting. Monitored performance using evaluation metrics like accuracy and loss curves.\\n\\nResume Screener POC with Chroma DB and LangChain: Developed a Proof of Concept (POC) for an intelligent resume screening application. Used Chroma DB for efficient document storage, indexing, and retrieval. Integrated LangChain for natural language processing (NLP) to analyze and rank resumes based on keyword relevance and content similarity. Designed a scoring system to prioritize resumes for candidate shortlisting. The POC automated resume filtering, reducing manual effort and increasing recruitment efficiency.\\n\\nProgramming Skills\\n\\nCloud & Tools: Azure Cloud (ADF, Databricks), GCP (Airflow, Dataproc, Looker), SQL Server, Power Platform\\n\\nProgramming Languages: Python, SQL, C#, JavaScript, Java • Web Development: ASP.NET Core MVC, Razor Views, HTML5, CSS3, Bootstrap, jQuery • Databases & ORM: SQL Server, Entity Framework Core, LINQ, Code-First Migrations • Data Warehousing & ETL: Data Modeling, Pipeline Orchestration, Apache Airflow, SparkSQL, PySpark • BI & Analytics: Power BI, Tableau, Looker • Machine Learning & AI: TensorFlow, Keras, LangChain, Chroma DB, Neural Networks, Data Certifications • Databricks Generative AI Fundamentals: Databricks — Valid through August 2026 • AI A-Z: Artificial Intelligence: Udemy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manideep S\\AppData\\Local\\Temp\\ipykernel_9492\\4095360967.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
      "OllamaEmbeddings: 100%|██████████| 2/2 [00:35<00:00, 17.81s/it]\n"
     ]
    }
   ],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED   \n",
      "mistral:latest             f974a74358d6    4.1 GB    3 days ago    \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    3 days ago    \n"
     ]
    }
   ],
   "source": [
    "!Ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !Ollama pull mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manideep S\\AppData\\Local\\Temp\\ipykernel_9492\\3585356516.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=local_model)\n"
     ]
    }
   ],
   "source": [
    "#llm from Ollama\n",
    "local_model = \"mistral\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You arre a expert resume writer. You read each persons latest resumes retrieved from vector database and provice insights about the resumes. You will think like an interviewer and ask questions to candidates. You review the resumes and suggest edits to the resumes. and based on the questions you have asked, you will analyse the resume and suggest changes that the candidate has knowledge true to his resume. Original question: {question}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(vector_db.as_retriever(),\n",
    "                                         llm,\n",
    "                                         prompt=QUERY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\":retriever, \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:10<00:00, 10.14s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Name of the individual: Manideep Sriperambudhuru\\n2. LinkedIn profile URL: manideepsp\\n3. Email address: manideepsp.16@gmail.com\\n4. Mobile number: +91 6300-519-47\\n5. Current job title (from resume): Not explicitly mentioned in the provided information.\\n6. Education: Not provided in the given data.\\n7. Key Skills (from Programming Skills section of the resume):\\n   - Cloud & Tools: Azure Cloud (ADF, Databricks), GCP (Airflow, Dataproc, Looker), SQL Server, Power Platform\\n   - Programming Languages: Python, SQL, C#, JavaScript, Java\\n   - Web Development: ASP.NET Core MVC, Razor Views, HTML5, CSS3, Bootstrap, jQuery\\n   - Databases & ORM: SQL Server, Entity Framework Core, LINQ, Code-First Migrations\\n   - Data Warehousing & ETL: Data Modeling, Pipeline Orchestration, Apache Airflow, SparkSQL, PySpark\\n   - BI & Analytics: Power BI, Tableau, Looker\\n   - Machine Learning & AI: TensorFlow, Keras, LangChain, Chroma DB, Neural Networks\\n   - Additional Skills/Certifications: Databricks Generative AI Fundamentals (Valid through August 2026), AI A-Z (Artificial Intelligence) from Udemy.\\n8. Projects mentioned in the provided data (without details):\\n   - Resume Screener POC with Chroma DB and LangChain\\n   - Sentiment Analysis with ANN (IMDB Dataset)\\n   - Image Classification with CNN (Custom Dataset).'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:08<00:00,  8.69s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.33s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.33s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Programming Skills: Azure Cloud (ADF, Databricks), GCP (Airflow, Dataproc, Looker), SQL Server, Power Platform; Python, SQL, C#, JavaScript, Java\\n2. Web Development: ASP.NET Core MVC, Entity Framework Core, Razor Views, Bootstrap, JavaScript\\n3. Database Management: Designing optimized database schema in SQL Server\\n4. ETL & BI Solutions: Azure Data Factory (ADF), Power BI, Tableau, Google Cloud Platform (GCP) services like Dataproc and Looker\\n5. Machine Learning & AI: Artificial Neural Networks (ANN) using TensorFlow and Keras, Convolutional Neural Networks (CNN)\\n6. Additional Skills: Troubleshooting complex data issues, performance optimization strategies, documentation, knowledge sharing, and end-to-end system design'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are the key skills of the candidate?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
